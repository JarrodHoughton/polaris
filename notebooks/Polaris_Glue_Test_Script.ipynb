{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8485c-6235-4595-87fa-dd4664ceaa6c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae48fc9d-dd2c-498d-88f3-7180aa5f1bbd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/glue_user/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/glue_user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/glue_user/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.4_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9afd3582-f072-4bfa-9654-9ac5c48a5b3c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.4_2.12;1.6.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-aws;1.6.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-api;1.6.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound org.apache.iceberg#iceberg-bundled-guava;1.6.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-common;1.6.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-core;1.6.1 in central\n",
      "\tfound org.apache.avro#avro;1.11.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.14.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.14.2 in central\n",
      "\tfound org.apache.commons#commons-compress;1.22 in central\n",
      "\tfound io.airlift#aircompressor;0.27 in central\n",
      "\tfound org.apache.httpcomponents.client5#httpclient5;5.3.1 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.2.4 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.2.4 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.19.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.10.0 in central\n",
      "\tfound org.roaringbitmap#RoaringBitmap;1.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.4.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.23.19 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.1.3.Final in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.23.19 in central\n",
      "\tfound software.amazon.awssdk#utils;2.23.19 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.23.19 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.23.19 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.23.19 in central\n",
      ":: resolution report :: resolve 568ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.14.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.14.2 from central in [default]\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.3 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.10.0 from central in [default]\n",
      "\tio.airlift#aircompressor;0.27 from central in [default]\n",
      "\torg.apache.avro#avro;1.11.3 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.22 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.4.0 from central in [default]\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.3.1 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.2.4 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.2.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-api;1.6.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-aws;1.6.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-bundled-guava;1.6.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-common;1.6.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-core;1.6.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.4_2.12;1.6.1 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.19.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.roaringbitmap#RoaringBitmap;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.1.3.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.23.19 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.23.19 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.23.19 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.23.19 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.23.19 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.23.19 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.30 by [org.slf4j#slf4j-api;1.7.36] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   30  |   0   |   0   |   1   ||   29  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9afd3582-f072-4bfa-9654-9ac5c48a5b3c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 29 already retrieved (0kB/10ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/home/glue_user/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "24/09/07 01:28:34 WARN Job$: Job run ID Polaris_Test is either null or empty or its same as Job name. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception while setting endpoint config: java.lang.NullPointerException\n",
      "Spark Running\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Client ID and Secret for the root principal in Polaris\n",
    "client_id = '31f0fb0a62307999'\n",
    "client_secret = '792dadce3f3e05219c03340d29d7f09f'\n",
    "POLARIS_URI = 'http://polaris:8181/api/catalog'\n",
    "POLARIS_CATALOG_NAME = 'polaris_demo'\n",
    "POLARIS_WAREHOUSE_DIR = 's3://polaris-test-data-lake/polaris_test/'\n",
    "POLARIS_CREDENTIALS = f'{client_id}:{client_secret}'\n",
    "POLARIS_SCOPE = 'PRINCIPAL_ROLE:ALL'\n",
    "\n",
    "# Arguments for the job\n",
    "args = {\n",
    "    \"JOB_NAME\": \"Polaris_Test\"\n",
    "}\n",
    "\n",
    "# Configure SparkConf with custom settings\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('Polaris_Test')\n",
    "  \t\t#packages\n",
    "        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.6.1,org.apache.iceberg:iceberg-aws:1.6.1,org.apache.hadoop:hadoop-aws:3.4.0,software.amazon.awssdk:bundle:2.23.19,software.amazon.awssdk:url-connection-client:2.23.19')\n",
    "  \t\t#SQL Extensions\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
    "        #Configuring Catalog\n",
    "        .set('spark.sql.catalog.polaris', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.polaris.warehouse', POLARIS_CATALOG_NAME)\n",
    "        .set('spark.sql.warehouse.dir', POLARIS_WAREHOUSE_DIR)\n",
    "        .set(\"spark.sql.catalog.polaris.s3.region\", \"eu-west-1\")\n",
    "        .set('spark.sql.catalog.polaris.catalog-impl', 'org.apache.iceberg.rest.RESTCatalog')\n",
    "        .set('spark.sql.catalog.solid_dose.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        .set('spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation', 'true')\n",
    "        .set('spark.sql.catalog.polaris.uri', POLARIS_URI)\n",
    "        .set('spark.sql.catalog.polaris.credential', POLARIS_CREDENTIALS)\n",
    "        .set('spark.sql.catalog.arctic.authentication.type','BEARER')\n",
    "        .set('spark.sql.catalog.polaris.scope', POLARIS_SCOPE)\n",
    "        .set('spark.sql.catalog.polaris.token-refresh-enabled', 'true')\n",
    "        .set(\"spark.eventLog.enabled\", \"true\")\n",
    "        # .set('spark.sql.defaultCatalog','polaris')\n",
    ")\n",
    "\n",
    "# Initialize Spark Session using SparkConf\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GlueJob\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize GlueContext\n",
    "glueContext = GlueContext(spark.sparkContext)\n",
    "\n",
    "# spark = glueContext.spark_session\n",
    "\n",
    "# Initialize Job in Glue\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "print(\"Spark Running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d10a7c1-12f1-42f7-a13e-cea13572d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ea29c-ba98-42c2-981b-05183533df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ce951c-dd0a-45bb-9a49-23f6740a4d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 01:29:01 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|catalog|\n",
      "+-------+\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a889d7-2055-4f2f-b867-16dec48341c8",
   "metadata": {},
   "source": [
    "# Use Polaris\n",
    "Tell spark to use polaris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc06a706-e80d-400d-9f40-bf15efa8c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/07 01:29:17 INFO HiveConf: Found configuration file file:/home/glue_user/spark/conf/hive-site.xml\n",
      "24/09/07 01:29:26 WARN EC2MetadataUtils: Unable to retrieve the requested metadata (/latest/dynamic/instance-identity/document). Failed to connect to service endpoint: \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:70)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:75)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1605)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1594)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:615)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:248)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:248)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:151)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:141)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listDatabases(SessionCatalog.scala:294)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.listNamespaces(V2SessionCatalog.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:222)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:102)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.SocketTimeoutException: connect timed out\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:463)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:558)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:242)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:339)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:357)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 85 more\n",
      "24/09/07 01:29:26 INFO AWSGlueClientFactory: No region info found, using SDK default region: us-east-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     namespace|\n",
      "+--------------+\n",
      "|afrocentric_db|\n",
      "|       default|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"USE polaris\")\n",
    "spark.sql(\"SHOW NAMESPACES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b6f5e4-657a-418d-8a9f-1762b67e3dad",
   "metadata": {},
   "source": [
    "# Create Nested Namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984eec1-323e-4919-a2a9-0ee6cd5142cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS cloudza_test\")\n",
    "spark.sql(\"USE CLOUDZA_TEST\")\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS cloudza_test.public\")\n",
    "spark.sql(\"SHOW NAMESPACES IN cloudza_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2df00a-f08e-486d-8429-ad542411126f",
   "metadata": {},
   "source": [
    "# Create a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f10f8c-5518-4147-b5ca-bc5624a4f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE NAMESPACE cloudza_test\")\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS cloudza_test.test_table(\n",
    "    id bigint NOT NULL COMMENT 'unique id',\n",
    "    data string)\n",
    "USING iceberg;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcab253-a140-4a79-b619-fdf6a7e9d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c1fb6-bf6d-4933-aefc-b0dbffddae9d",
   "metadata": {},
   "source": [
    "# It's Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50695124-b753-4711-808d-90d02add28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE NAMESPACE cloudza_test\")\n",
    "spark.sql(\"SELECT * FROM cloudza_test.test_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e82e8-7d4d-4509-a021-d9d286e289db",
   "metadata": {},
   "source": [
    "# Insert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a939b-8486-41fc-8138-b8a519fcadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a983043-36ca-4b8b-8597-ac60f7b30add",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO TEST_TABLE VALUES (1, 'some data'), (2, 'more data'), (3, 'yet more data')\")\n",
    "spark.sql(\"SELECT * FROM TEST_TABLE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a3bb8-4d40-4e94-83ce-2614ae6e97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
